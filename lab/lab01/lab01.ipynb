{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMW7GaIYQhUmRKcuywqpAUc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Handling Missing Data"],"metadata":{"id":"sTO9LnrW-_vU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsCc0awd-6gx"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Create sample data with missing values\n","data = {'Age': [25, 30, np.nan, 35, 40],\n","        'Income': [50000, 60000, 70000, np.nan, 90000]}\n","\n","df = pd.DataFrame(data)\n","print(\"Original Data:\\n\", df)\n","\n","# Imputation with mean for missing values\n","df['Age'].fillna(df['Age'].mean(), inplace=True)\n","df['Income'].fillna(df['Income'].mean(), inplace=True)\n","\n","print(\"\\nData After Imputation:\\n\", df)\n"]},{"cell_type":"markdown","source":["# Detecting and Handling Outliers"],"metadata":{"id":"4xmjiVF0_FLV"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","# Create sample data\n","data = [10, 20, 30, 40, 100]\n","df = pd.DataFrame(data, columns=['Value'])\n","\n","# Calculate IQR (Interquartile Range)\n","Q1 = df['Value'].quantile(0.25)\n","Q3 = df['Value'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Determine outlier threshold\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","print(\"Lower Bound:\", lower_bound)\n","print(\"Upper Bound:\", upper_bound)\n","\n","# Filter out outliers\n","df_filtered = df[(df['Value'] >= lower_bound) & (df['Value'] <= upper_bound)]\n","print(\"\\nFiltered Data (Outliers Removed):\\n\", df_filtered)\n"],"metadata":{"id":"Djj_GHu1_52U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Scaling - Normalization"],"metadata":{"id":"SNNHLZR6_7ey"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","# Create sample data\n","data = np.array([2, 8, 5, 10]).reshape(-1, 1)\n","\n","# Apply Min-Max normalization\n","scaler = MinMaxScaler()\n","normalized_data = scaler.fit_transform(data)\n","\n","print(\"Normalized Data:\", normalized_data)\n"],"metadata":{"id":"41DoC48-_-uU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Scaling - Standardization"],"metadata":{"id":"kvnRwMwmAAac"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Create sample data\n","data = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n","\n","# Apply Standardization (Z-Score Normalization)\n","scaler = StandardScaler()\n","standardized_data = scaler.fit_transform(data)\n","\n","print(\"Standardized Data:\", standardized_data)\n"],"metadata":{"id":"XF5tnfoWADHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encoding Categorical Data - One-Hot Encoding"],"metadata":{"id":"luZ7ZpHuAFEj"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Create a DataFrame with a categorical column\n","df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']})\n","\n","# Perform One-Hot Encoding\n","one_hot_encoded = pd.get_dummies(df['Color'], prefix='Color')\n","\n","print(\"One-Hot Encoded Data:\\n\", one_hot_encoded)\n"],"metadata":{"id":"nqkmbdQrAHnh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encoding Categorical Data - Label Encoding"],"metadata":{"id":"LGIYGfi6AJ5x"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Create sample data\n","df = pd.DataFrame({'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small']})\n","\n","# Initialize the label encoder\n","label_encoder = LabelEncoder()\n","\n","# Perform Label Encoding\n","df['Size_Encoded'] = label_encoder.fit_transform(df['Size'])\n","\n","print(\"Label Encoded Data:\\n\", df)\n"],"metadata":{"id":"KIVnDO12AMhE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Selection - Filter Methods"],"metadata":{"id":"ENMbmWpvAOYX"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Create a sample dataframe with two features\n","data = {'Feature1': [1, 2, 3, 4, 5],\n","        'Feature2': [5, 4, 3, 2, 1],\n","        'Feature3': [2, 3, 4, 5, 6]}\n","df = pd.DataFrame(data)\n","\n","# Compute correlation matrix\n","correlation_matrix = df.corr()\n","\n","print(\"Correlation Matrix:\\n\", correlation_matrix)\n"],"metadata":{"id":"jyyiSKdPAQx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Splitting - Training and Test Sets"],"metadata":{"id":"AuvSkzjGASa5"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Create sample data\n","X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n","y = np.array([0, 1, 0, 1, 0, 1])\n","\n","# Split data into training and test sets (80%/20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"Training Data (X):\\n\", X_train)\n","print(\"Test Data (X):\\n\", X_test)\n"],"metadata":{"id":"eZK5FODsAVGI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"HrF8rb08AWqs"}}]}