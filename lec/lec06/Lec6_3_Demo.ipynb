{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udnU7EWGOAfl"
      },
      "source": [
        "# Case Study: Decision Tree Pruning for Iris Dataset\n",
        "\n",
        "In this notebook, we will explore the concepts of **pre-pruning** and **post-pruning** in decision trees. We will use the Iris dataset and evaluate the performance of decision trees with pre-pruning (restricting the depth and splitting criteria) and post-pruning (cost-complexity pruning). We will then compare the results based on training and testing accuracy."
      ],
      "id": "udnU7EWGOAfl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e10644",
      "metadata": {
        "id": "d1e10644"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zuwUyDOAfn"
      },
      "source": [
        "### Load Dataset\n",
        "\n",
        "We will use the Iris dataset, a classic dataset for classification tasks. It contains 150 samples of iris flowers, categorized into three species: setosa, versicolor, and virginica, based on four features."
      ],
      "id": "P1zuwUyDOAfn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a13f111",
      "metadata": {
        "id": "8a13f111"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYOgxTP8OAfo"
      },
      "source": [
        "### Split Data into Training and Test Sets\n",
        "\n",
        "We will split the data into training (80%) and testing (20%) sets for evaluation."
      ],
      "id": "FYOgxTP8OAfo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b80decb",
      "metadata": {
        "id": "5b80decb"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nct_tOrOAfp"
      },
      "source": [
        "### Pre-pruning Decision Tree\n",
        "\n",
        "Pre-pruning is done by setting restrictions during tree construction, such as limiting the maximum depth or the minimum number of samples required to split a node. Here, we set the maximum depth to 3 and the minimum number of samples required to split a node to 5."
      ],
      "id": "4nct_tOrOAfp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c07e5a4",
      "metadata": {
        "id": "2c07e5a4"
      },
      "outputs": [],
      "source": [
        "# Train a decision tree classifier with pre-pruning\n",
        "clf_preprune = DecisionTreeClassifier(max_depth=3, min_samples_split=5)\n",
        "clf_preprune.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_zNfcfQOAfp"
      },
      "source": [
        "### Evaluate Pre-pruned Model\n",
        "\n",
        "We evaluate the performance of the pre-pruned decision tree on both the training and testing sets."
      ],
      "id": "K_zNfcfQOAfp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc215cf7",
      "metadata": {
        "id": "fc215cf7"
      },
      "outputs": [],
      "source": [
        "# Evaluate pre-pruned model\n",
        "train_acc_preprune = clf_preprune.score(X_train, y_train)\n",
        "test_acc_preprune = clf_preprune.score(X_test, y_test)\n",
        "\n",
        "train_acc_preprune, test_acc_preprune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPUZlGGWOAfq"
      },
      "source": [
        "### Post-pruning Decision Tree (Cost-Complexity Pruning)\n",
        "\n",
        "Post-pruning is done after the tree is fully grown by removing branches that add little value to the model. This is typically done using cost-complexity pruning. Here, we apply post-pruning by setting a value for `ccp_alpha`."
      ],
      "id": "nPUZlGGWOAfq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39a32f3",
      "metadata": {
        "id": "d39a32f3"
      },
      "outputs": [],
      "source": [
        "# Perform post-pruning using cost-complexity pruning\n",
        "clf_postprune = DecisionTreeClassifier(ccp_alpha=0.01)\n",
        "clf_postprune.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ30WpzeOAfq"
      },
      "source": [
        "### Evaluate Post-pruned Model\n",
        "\n",
        "We evaluate the performance of the post-pruned decision tree on both the training and testing sets."
      ],
      "id": "yJ30WpzeOAfq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f74a6b93",
      "metadata": {
        "id": "f74a6b93"
      },
      "outputs": [],
      "source": [
        "# Evaluate post-pruned model\n",
        "train_acc_postprune = clf_postprune.score(X_train, y_train)\n",
        "test_acc_postprune = clf_postprune.score(X_test, y_test)\n",
        "\n",
        "train_acc_postprune, test_acc_postprune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04He2TB8OAfq"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "The results show the accuracy of both pre-pruned and post-pruned decision trees. By controlling the depth and splitting criteria, pre-pruning reduces the complexity of the model, while post-pruning aims to simplify the tree further after it has been built.\n",
        "\n",
        "We can use the following output to compare the performance:\n",
        "- **Pre-pruning**: Train and test accuracy\n",
        "- **Post-pruning**: Train and test accuracy"
      ],
      "id": "04He2TB8OAfq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3e65ff",
      "metadata": {
        "id": "bb3e65ff"
      },
      "outputs": [],
      "source": [
        "print(f\"Pre-pruning: Train Accuracy = {train_acc_preprune}, Test Accuracy = {test_acc_preprune}\")\n",
        "print(f\"Post-pruning: Train Accuracy = {train_acc_postprune}, Test Accuracy = {test_acc_postprune}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}