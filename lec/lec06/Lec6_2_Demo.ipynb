{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w91LiMDsHM-I"
      },
      "source": [
        "# Case Study: Fruit Classification using Decision Trees\n",
        "\n",
        "In this notebook, we will use the `DecisionTreeClassifier` to classify fruits based on their weight (Feature1) and sweetness level (Feature2). We will compare the performance of decision trees using two different criteria: **Entropy (Information Gain)** and **Gini Index**. We will also calculate the accuracy and classification error for both classifiers.\n",
        "\n",
        "### Steps:\n",
        "1. Generate random data for features and labels.\n",
        "2. Split the data into training and testing sets.\n",
        "3. Train two decision tree classifiers using different criteria.\n",
        "4. Evaluate the performance using accuracy and classification error.\n",
        "5. Visualize the decision trees."
      ],
      "id": "w91LiMDsHM-I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6adf99",
      "metadata": {
        "id": "cd6adf99"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoDxQjB4HM-K"
      },
      "source": [
        "### Generating Data\n",
        "\n",
        "We generate 100 random data points for fruit classification based on their weight and sweetness level."
      ],
      "id": "hoDxQjB4HM-K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6bd4fa",
      "metadata": {
        "id": "2e6bd4fa"
      },
      "outputs": [],
      "source": [
        "# Generate random data for Feature1, Feature2, and Label (Fruits)\n",
        "np.random.seed(42)\n",
        "feature1 = np.random.randint(100, 300, size=100)  # Random weight (grams) between 100 and 300\n",
        "feature2 = np.random.randint(1, 10, size=100)     # Random sweetness level between 1 and 10\n",
        "\n",
        "# Label fruits based on a condition (e.g., weight and sweetness)\n",
        "labels = np.where((feature1 > 200) & (feature2 > 5), 'Apple', 'Orange')\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Feature1': feature1,  # Weight (grams)\n",
        "    'Feature2': feature2,  # Sweetness level\n",
        "    'Label': labels\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3j3iiPDHM-L"
      },
      "source": [
        "### Splitting the Data into Training and Test Sets\n",
        "\n",
        "We split the data into training (70%) and testing (30%) sets."
      ],
      "id": "u3j3iiPDHM-L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b10b6bb",
      "metadata": {
        "id": "3b10b6bb"
      },
      "outputs": [],
      "source": [
        "# Features and labels\n",
        "X = df[['Feature1', 'Feature2']].values\n",
        "y = df['Label'].values\n",
        "\n",
        "# Split data into training and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-TBKxdVHM-M"
      },
      "source": [
        "### Training Decision Trees\n",
        "\n",
        "We train two decision tree classifiers, one using the **Entropy** criterion and the other using the **Gini Index**."
      ],
      "id": "6-TBKxdVHM-M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b9b1ff",
      "metadata": {
        "id": "91b9b1ff"
      },
      "outputs": [],
      "source": [
        "# Decision Tree Classifier with Entropy (Information Gain)\n",
        "clf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Decision Tree Classifier with Gini Index\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_gini.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFpa2WjVHM-N"
      },
      "source": [
        "### Prediction and Evaluation\n",
        "\n",
        "We predict the labels on the test set using both models, and calculate the **accuracy** and **classification error**."
      ],
      "id": "pFpa2WjVHM-N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087bbfe0",
      "metadata": {
        "id": "087bbfe0"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "y_pred_gini = clf_gini.predict(X_test)\n",
        "\n",
        "# Accuracy Calculation\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
        "\n",
        "# Classification Error Calculation\n",
        "classification_error_entropy = 1 - accuracy_entropy\n",
        "classification_error_gini = 1 - accuracy_gini\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy (Entropy): {accuracy_entropy}\")\n",
        "print(f\"Classification Error (Entropy): {classification_error_entropy}\")\n",
        "print(f\"Accuracy (Gini): {accuracy_gini}\")\n",
        "print(f\"Classification Error (Gini): {classification_error_gini}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0d8gI5dHM-N"
      },
      "source": [
        "### Visualizing the Decision Trees\n",
        "\n",
        "We visualize the decision trees for both classifiers."
      ],
      "id": "M0d8gI5dHM-N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf43270",
      "metadata": {
        "id": "fcf43270"
      },
      "outputs": [],
      "source": [
        "# Visualize the trees\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "tree.plot_tree(clf_entropy, filled=True, ax=ax[0])\n",
        "ax[0].set_title(\"Decision Tree (Entropy)\")\n",
        "\n",
        "tree.plot_tree(clf_gini, filled=True, ax=ax[1])\n",
        "ax[1].set_title(\"Decision Tree (Gini)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKmONIobHM-O"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "- **Accuracy**: Both the Entropy and Gini models achieved an accuracy of 93.33%.\n",
        "- **Classification Error**: Both models have a classification error of 6.67%.\n",
        "\n",
        "The decision trees visualized above show how the features split the data to classify the fruits as either \"Apple\" or \"Orange\"."
      ],
      "id": "uKmONIobHM-O"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}