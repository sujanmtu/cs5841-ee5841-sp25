{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utaXqN7mzesl"
      },
      "source": [
        "# Case Study: Bagging and Boosting Approach for Fault Detection in Electrical Systems\n",
        "\n",
        "In this notebook, we explore **Ensemble Learning** techniques: **Bagging** and **Boosting**. We will use **Decision Trees** as base models and compare their performance on synthetic data. These techniques help improve the performance of weak models by combining them into a strong one.\n",
        "\n",
        "## Steps:\n",
        "1. **Generate Synthetic Data**\n",
        "2. **Bagging with Decision Tree**\n",
        "3. **Boosting with Decision Tree**\n",
        "4. **Compare Performance**"
      ],
      "id": "utaXqN7mzesl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_EfYKNizesm"
      },
      "source": [
        "### Step 1: Install/load Required Libraries\n",
        "\n"
      ],
      "id": "l_EfYKNizesm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqWqgyVzzesn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "DqWqgyVzzesn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkla8Gmqzeso"
      },
      "source": [
        "### Step 2: Generate Synthetic Data\n",
        "\n",
        "We will generate synthetic data for classification, simulating a **fault detection** scenario in electrical systems. This dataset will have 20 features, of which 10 are informative."
      ],
      "id": "Qkla8Gmqzeso"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I79Ao_Gpzeso"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic classification data (simulating electrical system data)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Create a DataFrame\n",
        "feature_names = [f'Feature_{i+1}' for i in range(X.shape[1])]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['Target'] = y  # Add target column\n",
        "\n",
        "# Display first five rows\n",
        "df.head()"
      ],
      "id": "I79Ao_Gpzeso"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9IIkQw5CM8lm"
      },
      "id": "9IIkQw5CM8lm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzefvc1ozeso"
      },
      "source": [
        "### Step 3: Bagging with Decision Tree\n",
        "\n",
        "Here we apply **Bagging** using **Decision Trees** as the base model. **Bagging** trains multiple models independently on bootstrapped data (random sampling with replacement) and combines their predictions."
      ],
      "id": "vzefvc1ozeso"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koWbi43Uzeso"
      },
      "outputs": [],
      "source": [
        "# Train a Bagging Classifier with Decision Tree as the base model\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "print('Bagging (with Decision Tree) Accuracy: ', accuracy_score(y_test, bagging_pred))\n"
      ],
      "id": "koWbi43Uzeso"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccr7o66szesp"
      },
      "source": [
        "### Step 4: Boosting with Decision Tree\n",
        "\n",
        "Now, we will use **Boosting** with **Decision Trees** as the base model. In **Boosting**, models are trained sequentially, where each model corrects the errors made by the previous one."
      ],
      "id": "ccr7o66szesp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y658MTVzesp"
      },
      "outputs": [],
      "source": [
        "# Train a Gradient Boosting Classifier with Decision Tree as the base model\n",
        "boosting = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "boosting.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "boosting_pred = boosting.predict(X_test)\n",
        "print('Boosting (with Decision Tree) Accuracy: ', accuracy_score(y_test, boosting_pred))"
      ],
      "id": "7Y658MTVzesp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTzAnXA0zesp"
      },
      "source": [
        "### Step 5: Compare Performance\n",
        "\n",
        "Letâ€™s compare the performance of **Bagging** and **Boosting** based on accuracy."
      ],
      "id": "aTzAnXA0zesp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X59AlM44zesp"
      },
      "outputs": [],
      "source": [
        "# Output comparison\n",
        "print(f'Bagging (with Decision Tree) Accuracy: {accuracy_score(y_test, bagging_pred):.4f}')\n",
        "print(f'Boosting (with Decision Tree) Accuracy: {accuracy_score(y_test, boosting_pred):.4f}')"
      ],
      "id": "X59AlM44zesp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P4j0K4Ezesq"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "- **Bagging** reduces **variance** by training multiple models independently, but it does not significantly improve **bias**.\n",
        "- **Boosting** reduces **bias** by focusing on errors from previous models, but it can be more prone to overfitting compared to **Bagging**.\n",
        "\n",
        "In our case study on synthetic electrical fault detection, we observed that **Boosting** provided better performance compared to **Bagging**, particularly for more complex patterns in the data.\n",
        "\n",
        "---\n",
        "\n",
        "This notebook demonstrates the implementation of **Bagging** and **Boosting** using **Decision Trees** and their performance comparison. You can easily modify the dataset or models to suit real-world electrical engineering problems or other domains as needed."
      ],
      "id": "9P4j0K4Ezesq"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}