{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrkCv7xYzgZi"
      },
      "source": [
        "# Case Study: PID Parameter Prediction using Random Forest\n",
        "\n",
        "In this case study, we explore the use of machine learning to predict the PID parameters (Kp, Ki, Kd) for a PID controller based on simulated system data. We generate random data for the controller's error, derivative of error, integral of error, and setpoint. These features are then used to predict the PID gains (Kp, Ki, Kd) using two machine learning models: Decision Tree Regressor and Random Forest Regressor.\n",
        "\n",
        "We evaluate the performance of both models by calculating their R-squared (R²) scores and visualizing the predicted PID parameters against the true values."
      ],
      "id": "JrkCv7xYzgZi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpACxO4nzgZk"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "In this section, we import the necessary libraries for data manipulation, machine learning models, and plotting."
      ],
      "id": "hpACxO4nzgZk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a57b23d",
      "metadata": {
        "id": "1a57b23d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42H0ze8UzgZl"
      },
      "source": [
        "## Simulate Data for PID Parameters\n",
        "\n",
        "In this section, we simulate the data for the features (Error, Derivative, Integral, Setpoint) and the target variables (Kp, Ki, Kd) used for training the machine learning models."
      ],
      "id": "42H0ze8UzgZl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f4b194",
      "metadata": {
        "id": "36f4b194"
      },
      "outputs": [],
      "source": [
        "# Simulate the data for PID controller tuning\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random data for features: Error (e), Derivative (de/dt), Integral (∫e), Setpoint\n",
        "n_samples = 1000\n",
        "error = np.random.uniform(-2, 2, n_samples)  # Error in temperature\n",
        "derivative = np.random.uniform(-0.5, 0.5, n_samples)  # Rate of change of error\n",
        "integral = np.cumsum(error)  # Cumulative error over time (integral)\n",
        "setpoint = 25 + np.random.normal(0, 0.5, n_samples)  # Target setpoint with noise\n",
        "\n",
        "# Generate PID parameters as target variables (Kp, Ki, Kd)\n",
        "Kp = 2 + 0.5 * error + 0.1 * derivative\n",
        "Ki = 0.1 + 0.05 * integral\n",
        "Kd = 0.02 + 0.01 * derivative\n",
        "\n",
        "# Create a DataFrame with features and target variables\n",
        "data = pd.DataFrame({\n",
        "    'Error': error,\n",
        "    'Derivative': derivative,\n",
        "    'Integral': integral,\n",
        "    'Setpoint': setpoint,\n",
        "    'Kp': Kp,\n",
        "    'Ki': Ki,\n",
        "    'Kd': Kd\n",
        "})\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V10YnbDYzgZm"
      },
      "source": [
        "## Prepare Features and Target Variables\n",
        "\n",
        "Here, we separate the features (X) from the target variables (y) and normalize the features to ensure that they are on a similar scale, improving model performance."
      ],
      "id": "V10YnbDYzgZm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bfa5f6d",
      "metadata": {
        "id": "1bfa5f6d"
      },
      "outputs": [],
      "source": [
        "# Features (X) and Target variables (y)\n",
        "X = data[['Error', 'Derivative', 'Integral', 'Setpoint']]\n",
        "y = data[['Kp', 'Ki', 'Kd']]\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgkmPB2SzgZn"
      },
      "source": [
        "## Train Decision Tree and Random Forest Models\n",
        "\n",
        "In this section, we train the Decision Tree Regressor and Random Forest Regressor models on the training data."
      ],
      "id": "xgkmPB2SzgZn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d340fbf",
      "metadata": {
        "id": "3d340fbf"
      },
      "outputs": [],
      "source": [
        "# Decision Tree Model\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTF9mUVEzgZo"
      },
      "source": [
        "## Make Predictions\n",
        "\n",
        "We use the trained models to make predictions on the test set for the PID parameters (Kp, Ki, Kd)."
      ],
      "id": "XTF9mUVEzgZo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31c88c1",
      "metadata": {
        "id": "c31c88c1"
      },
      "outputs": [],
      "source": [
        "# Make Predictions\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "rf_predictions = rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo3-HNDkzgZp"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "We evaluate the performance of both models using the R-squared (R²) score, which measures how well the models predict the PID parameters. A higher R² score indicates better model performance."
      ],
      "id": "jo3-HNDkzgZp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9e706e",
      "metadata": {
        "id": "cd9e706e"
      },
      "outputs": [],
      "source": [
        "# Calculate R-squared (R²) for each parameter (Kp, Ki, Kd)\n",
        "dt_r2_kp = r2_score(y_test['Kp'], dt_predictions[:, 0])\n",
        "rf_r2_kp = r2_score(y_test['Kp'], rf_predictions[:, 0])\n",
        "\n",
        "dt_r2_ki = r2_score(y_test['Ki'], dt_predictions[:, 1])\n",
        "rf_r2_ki = r2_score(y_test['Ki'], rf_predictions[:, 1])\n",
        "\n",
        "dt_r2_kd = r2_score(y_test['Kd'], dt_predictions[:, 2])\n",
        "rf_r2_kd = r2_score(y_test['Kd'], rf_predictions[:, 2])\n",
        "\n",
        "# Print the R-squared values for each parameter\n",
        "print('Decision Tree Model:')\n",
        "print('R-squared (Kp):', dt_r2_kp)\n",
        "print('R-squared (Ki):', dt_r2_ki)\n",
        "print('R-squared (Kd):', dt_r2_kd)\n",
        "\n",
        "print('\\nRandom Forest Model:')\n",
        "print('R-squared (Kp):', rf_r2_kp)\n",
        "print('R-squared (Ki):', rf_r2_ki)\n",
        "print('R-squared (Kd):', rf_r2_kd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxfADjhazgZp"
      },
      "source": [
        "## Visualize Model Predictions\n",
        "\n",
        "We visualize the predictions for the PID parameters (Kp, Ki, Kd) made by both the Decision Tree and Random Forest models. The scatter plots display the true values vs predicted values for each parameter."
      ],
      "id": "ZxfADjhazgZp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c6d94f",
      "metadata": {
        "id": "a8c6d94f"
      },
      "outputs": [],
      "source": [
        "# Visualize the predictions vs true values for each parameter\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Plot Decision Tree Predictions for Kp, Ki, Kd\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(range(len(y_test)), y_test['Kp'], color='blue', label='True Kp', alpha=0.5)\n",
        "plt.scatter(range(len(y_test)), dt_predictions[:, 0], color='red', label='Predicted Kp', alpha=0.5)\n",
        "plt.title('Decision Tree - Kp Prediction')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Kp')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(range(len(y_test)), y_test['Ki'], color='blue', label='True Ki', alpha=0.5)\n",
        "plt.scatter(range(len(y_test)), dt_predictions[:, 1], color='red', label='Predicted Ki', alpha=0.5)\n",
        "plt.title('Decision Tree - Ki Prediction')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Ki')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(range(len(y_test)), y_test['Kd'], color='blue', label='True Kd', alpha=0.5)\n",
        "plt.scatter(range(len(y_test)), dt_predictions[:, 2], color='red', label='Predicted Kd', alpha=0.5)\n",
        "plt.title('Decision Tree - Kd Prediction')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Kd')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Peiwkf9dzgZp"
      },
      "source": [
        "## Random Forest Predictions\n",
        "\n",
        "In this section, we visualize the predictions made by the Random Forest model for each PID parameter (Kp, Ki, Kd)."
      ],
      "id": "Peiwkf9dzgZp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c7f415",
      "metadata": {
        "id": "53c7f415"
      },
      "outputs": [],
      "source": [
        "# Visualize the predictions vs true values for Random Forest\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Plot Random Forest Predictions for Kp, Ki, Kd\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(range(len(y_test)), y_test['Kp'], color='blue', label='True Kp', alpha=0.5)\n",
        "plt.scatter(range(len(y_test)), rf_predictions[:, 0], color='green', label='Predicted Kp', alpha=0.5)\n",
        "plt.title('Random Forest - Kp Prediction')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Kp')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(range(len(y_test)), y_test['Ki'], color='blue', label='True Ki', alpha=0.5)\n",
        "plt.scatter(range(len(y_test)), rf_predictions[:, 1], color='green', label='Predicted Ki', alpha=0.5)\n",
        "plt.title('Random Forest - Ki Prediction')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Ki')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(range(len(y_test)), y_test['Kd'], color='blue', label='True Kd', alpha=0.5)\n",
        "plt.scatter(range(len(y_test)), rf_predictions[:, 2], color='green', label='Predicted Kd', alpha=0.5)\n",
        "plt.title('Random Forest - Kd Prediction')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Kd')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}