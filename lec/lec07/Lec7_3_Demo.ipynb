{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtRpw1w2_Xhe"
      },
      "source": [
        "# Case Study: PID Controller Tuning using Boosting Models\n",
        "\n",
        "In this notebook, we will train and evaluate different boosting models (**AdaBoost**, **Gradient Boosting**, and **XGBoost**) to predict the PID controller parameters: `Kp`, `Ki`, and `Kd` based on system inputs. We will simulate the data for the features and targets, split it into training and testing sets, and then use three boosting algorithms for prediction.\n",
        "The notebook will demonstrate how each model performs and visualize the predictions compared to true values for the three parameters."
      ],
      "id": "xtRpw1w2_Xhe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWDzO_Sl_Xhg"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "XWDzO_Sl_Xhg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GepY7HCH_Xhh"
      },
      "source": [
        "### Simulating Data for PID Tuning\n",
        "In this section, we simulate the data for a PID controller. We generate random data for features like `Error`, `Derivative`, `Integral`, and `Setpoint`, and use these to create the target variables `Kp`, `Ki`, and `Kd`. The target variables represent the proportional, integral, and derivative gains, respectively, which are the parameters of the PID controller.\n"
      ],
      "id": "GepY7HCH_Xhh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8HlP4nV_Xhh"
      },
      "outputs": [],
      "source": [
        "# Simulate the data for PID controller tuning\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random data for features: Error (e), Derivative (de/dt), Integral (∫e), Setpoint\n",
        "n_samples = 1000\n",
        "error = np.random.uniform(-2, 2, n_samples)  # Error in temperature\n",
        "derivative = np.random.uniform(-0.5, 0.5, n_samples)  # Rate of change of error\n",
        "integral = np.cumsum(error)  # Cumulative error over time (integral)\n",
        "setpoint = 25 + np.random.normal(0, 0.5, n_samples)  # Target setpoint with noise\n",
        "\n",
        "# Generate PID parameters as target variables (Kp, Ki, Kd)\n",
        "Kp = 2 + 0.5 * error + 0.1 * derivative\n",
        "Ki = 0.1 + 0.05 * integral\n",
        "Kd = 0.02 + 0.01 * derivative\n",
        "\n",
        "# Create a DataFrame with features and target variables\n",
        "data = pd.DataFrame({\n",
        "    'Error': error,\n",
        "    'Derivative': derivative,\n",
        "    'Integral': integral,\n",
        "    'Setpoint': setpoint,\n",
        "    'Kp': Kp,\n",
        "    'Ki': Ki,\n",
        "    'Kd': Kd\n",
        "})\n",
        "data.head()"
      ],
      "id": "b8HlP4nV_Xhh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRocHqPy_Xhi"
      },
      "source": [
        "### Feature Scaling and Data Splitting\n",
        "Before training the models, we scale the features using `StandardScaler` and split the data into training and testing sets. The training set will be used to train the models, while the testing set will be used for evaluation.\n"
      ],
      "id": "iRocHqPy_Xhi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11WVHDFm_Xhi"
      },
      "outputs": [],
      "source": [
        "# Features (X) and Target variables (y)\n",
        "X = data[['Error', 'Derivative', 'Integral', 'Setpoint']]\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, data[['Kp', 'Ki', 'Kd']], test_size=0.2, random_state=42)"
      ],
      "id": "11WVHDFm_Xhi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtncRnwQ_Xhi"
      },
      "source": [
        "### Model Training and Prediction\n",
        "We will now train three separate models (AdaBoost, Gradient Boosting, and XGBoost) for each of the PID parameters (`Kp`, `Ki`, and `Kd`). After training the models, we will use them to make predictions on the test set.\n"
      ],
      "id": "VtncRnwQ_Xhi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bASyjhB_Xhi"
      },
      "outputs": [],
      "source": [
        "# Separate models for Kp, Ki, Kd\n",
        "# AdaBoost Regressor for Kp\n",
        "ada_model_kp = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "ada_model_kp.fit(X_train, y_train['Kp'])\n",
        "\n",
        "# AdaBoost Regressor for Ki\n",
        "ada_model_ki = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "ada_model_ki.fit(X_train, y_train['Ki'])\n",
        "\n",
        "# AdaBoost Regressor for Kd\n",
        "ada_model_kd = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "ada_model_kd.fit(X_train, y_train['Kd'])\n",
        "\n",
        "# Gradient Boosting Regressor for Kp\n",
        "gb_model_kp = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_model_kp.fit(X_train, y_train['Kp'])\n",
        "\n",
        "# Gradient Boosting Regressor for Ki\n",
        "gb_model_ki = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_model_ki.fit(X_train, y_train['Ki'])\n",
        "\n",
        "# Gradient Boosting Regressor for Kd\n",
        "gb_model_kd = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_model_kd.fit(X_train, y_train['Kd'])\n",
        "\n",
        "# XGBoost Regressor for Kp\n",
        "xg_model_kp = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "xg_model_kp.fit(X_train, y_train['Kp'])\n",
        "\n",
        "# XGBoost Regressor for Ki\n",
        "xg_model_ki = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "xg_model_ki.fit(X_train, y_train['Ki'])\n",
        "\n",
        "# XGBoost Regressor for Kd\n",
        "xg_model_kd = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "xg_model_kd.fit(X_train, y_train['Kd'])"
      ],
      "id": "5bASyjhB_Xhi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBWdqemk_Xhj"
      },
      "source": [
        "### Model Evaluation: R-squared (R²) Score\n",
        "We will evaluate the performance of each model using the R-squared (R²) score, which indicates the proportion of variance in the target variable that is explained by the model. The higher the R² score, the better the model.\n"
      ],
      "id": "aBWdqemk_Xhj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiDsyE-4_Xhj"
      },
      "outputs": [],
      "source": [
        "# Make Predictions for each model\n",
        "ada_predictions_kp = ada_model_kp.predict(X_test)\n",
        "ada_predictions_ki = ada_model_ki.predict(X_test)\n",
        "ada_predictions_kd = ada_model_kd.predict(X_test)\n",
        "\n",
        "gb_predictions_kp = gb_model_kp.predict(X_test)\n",
        "gb_predictions_ki = gb_model_ki.predict(X_test)\n",
        "gb_predictions_kd = gb_model_kd.predict(X_test)\n",
        "\n",
        "xg_predictions_kp = xg_model_kp.predict(X_test)\n",
        "xg_predictions_ki = xg_model_ki.predict(X_test)\n",
        "xg_predictions_kd = xg_model_kd.predict(X_test)\n",
        "\n",
        "# R-squared (R²) score for each model and target\n",
        "r2_ada_kp = r2_score(y_test['Kp'], ada_predictions_kp)\n",
        "r2_ada_ki = r2_score(y_test['Ki'], ada_predictions_ki)\n",
        "r2_ada_kd = r2_score(y_test['Kd'], ada_predictions_kd)\n",
        "r2_gb_kp = r2_score(y_test['Kp'], gb_predictions_kp)\n",
        "r2_gb_ki = r2_score(y_test['Ki'], gb_predictions_ki)\n",
        "r2_gb_kd = r2_score(y_test['Kd'], gb_predictions_kd)\n",
        "r2_xg_kp = r2_score(y_test['Kp'], xg_predictions_kp)\n",
        "r2_xg_ki = r2_score(y_test['Ki'], xg_predictions_ki)\n",
        "r2_xg_kd = r2_score(y_test['Kd'], xg_predictions_kd)\n",
        "\n",
        "# Display R² scores for each model and target\n",
        "print(f'AdaBoost R² for Kp: {r2_ada_kp:.2f}')\n",
        "print(f'AdaBoost R² for Ki: {r2_ada_ki:.2f}')\n",
        "print(f'AdaBoost R² for Kd: {r2_ada_kd:.2f}')\n",
        "print(f'Gradient Boosting R² for Kp: {r2_gb_kp:.2f}')\n",
        "print(f'Gradient Boosting R² for Ki: {r2_gb_ki:.2f}')\n",
        "print(f'Gradient Boosting R² for Kd: {r2_gb_kd:.2f}')\n",
        "print(f'XGBoost R² for Kp: {r2_xg_kp:.2f}')\n",
        "print(f'XGBoost R² for Ki: {r2_xg_ki:.2f}')\n",
        "print(f'XGBoost R² for Kd: {r2_xg_kd:.2f}')"
      ],
      "id": "WiDsyE-4_Xhj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxBWLiCD_Xhj"
      },
      "source": [
        "### Visualizing Predictions vs True Values\n",
        "We will now visualize the predictions made by each model against the true values for each of the PID parameters (`Kp`, `Ki`, `Kd`). This will allow us to visually compare the performance of the models."
      ],
      "id": "IxBWLiCD_Xhj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VqQJVU3_Xhj"
      },
      "outputs": [],
      "source": [
        "# Function to plot predictions vs true values\n",
        "def plot_predictions(true_values, predicted_values, title):\n",
        "    plt.scatter(true_values, predicted_values, alpha=0.5)\n",
        "    plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], color='red', linestyle='--')\n",
        "    plt.xlabel('True Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot for Kp\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Kp'], ada_predictions_kp, 'AdaBoost: Kp Predictions vs True Values')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Kp'], gb_predictions_kp, 'Gradient Boosting: Kp Predictions vs True Values')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Kp'], xg_predictions_kp, 'XGBoost: Kp Predictions vs True Values')\n",
        "\n",
        "# Plot for Ki\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Ki'], ada_predictions_ki, 'AdaBoost: Ki Predictions vs True Values')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Ki'], gb_predictions_ki, 'Gradient Boosting: Ki Predictions vs True Values')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Ki'], xg_predictions_ki, 'XGBoost: Ki Predictions vs True Values')\n",
        "\n",
        "# Plot for Kd\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Kd'], ada_predictions_kd, 'AdaBoost: Kd Predictions vs True Values')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Kd'], gb_predictions_kd, 'Gradient Boosting: Kd Predictions vs True Values')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_predictions(y_test['Kd'], xg_predictions_kd, 'XGBoost: Kd Predictions vs True Values')"
      ],
      "id": "2VqQJVU3_Xhj"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}