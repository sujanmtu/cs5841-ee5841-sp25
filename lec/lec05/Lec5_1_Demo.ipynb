{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: Predicting Loan Default using Logistic Regression"
      ],
      "metadata": {
        "id": "oqaHBPnM9u1j"
      },
      "id": "oqaHBPnM9u1j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPZ3D87l9gCE"
      },
      "source": [
        "### 1. Importing Necessary Libraries\n",
        "In this section, we import all the libraries required for the task. We use `numpy` for numerical operations, `pandas` for data handling, `sklearn` for machine learning functionalities, and `matplotlib` for data visualization."
      ],
      "id": "SPZ3D87l9gCE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Jfg8jCj9gCF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "8Jfg8jCj9gCF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH9hb0F39gCH"
      },
      "source": [
        "### 2. Data Generation\n",
        "Next, we generate synthetic data for the problem. We create features such as `Income`, `Age`, and `Credit Score`, as well as a binary target variable, `Loan Default`."
      ],
      "id": "PH9hb0F39gCH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI0FOjl99gCH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "# Generate random data\n",
        "n_samples = 1000\n",
        "income = np.random.randint(30000, 150000, size=n_samples)\n",
        "age = np.random.randint(18, 70, size=n_samples)\n",
        "credit_score = np.random.randint(600, 800, size=n_samples)\n",
        "loan_default = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])  # 30% chance of default\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Income': income,\n",
        "    'Age': age,\n",
        "    'Credit Score': credit_score,\n",
        "    'Loan Default': loan_default\n",
        "})"
      ],
      "id": "qI0FOjl99gCH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WVJle9p9gCI"
      },
      "source": [
        "### 3. Data Splitting\n",
        "After generating the data, we split it into training and testing sets. This helps evaluate the model on unseen data after training it on a portion of the data."
      ],
      "id": "4WVJle9p9gCI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuhOVn2p9gCI"
      },
      "outputs": [],
      "source": [
        "X = df[['Income', 'Age', 'Credit Score']]\n",
        "y = df['Loan Default']\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "id": "GuhOVn2p9gCI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUJfbk1P9gCI"
      },
      "source": [
        "### 4. Data Standardization\n",
        "Standardizing the data is crucial when using machine learning algorithms like Logistic Regression. This ensures that all features have a mean of 0 and a standard deviation of 1, which helps the model converge more efficiently."
      ],
      "id": "SUJfbk1P9gCI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9obFYmVq9gCJ"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "id": "9obFYmVq9gCJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaR5_ERk9gCJ"
      },
      "source": [
        "### 5. Model Initialization and Training\n",
        "We initialize the Logistic Regression model and train it using the standardized data. Logistic Regression is a simple yet effective algorithm for binary classification tasks."
      ],
      "id": "IaR5_ERk9gCJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axO_Vepi9gCJ"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)"
      ],
      "id": "axO_Vepi9gCJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZiv16O29gCJ"
      },
      "source": [
        "### 6. Model Evaluation\n",
        "After training the model, we evaluate its performance using metrics like accuracy, precision, recall, and F1-score. We also print the confusion matrix to understand the true positives, false positives, etc."
      ],
      "id": "QZiv16O29gCJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWpkNPrr9gCJ"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "# Display the results\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "id": "CWpkNPrr9gCJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2wdYsgz9gCJ"
      },
      "source": [
        "### 7. Visualizing the Decision Boundary\n",
        "Finally, we visualize the decision boundary of the trained model. This step helps us understand how the model is separating the two classes, `Loan Default` and `No Loan Default`, in the feature space."
      ],
      "id": "y2wdYsgz9gCJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUcG54J-9gCK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=plt.cm.Paired, edgecolor='k', s=100)\n",
        "plt.xlabel('Income (Standardized)')\n",
        "plt.ylabel('Age (Standardized)')\n",
        "plt.title('Decision Boundary for Loan Default Prediction')\n",
        "plt.colorbar(label='Loan Default (0 or 1)')\n",
        "plt.show()"
      ],
      "id": "UUcG54J-9gCK"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}