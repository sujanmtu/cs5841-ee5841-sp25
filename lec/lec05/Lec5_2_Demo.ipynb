{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: Wearable Sensor Data Analysis for Disease Detection"
      ],
      "metadata": {
        "id": "WLNmCZ5ifEnL"
      },
      "id": "WLNmCZ5ifEnL"
    },
    {
      "cell_type": "markdown",
      "id": "89534fe5",
      "metadata": {
        "id": "89534fe5"
      },
      "source": [
        "**Step-by-Step: Implementing Logistic Regression for Disease Classification Using Wearable Sensor Data**\n",
        "\n",
        "In this notebook, we will apply logistic regression to healthcare data collected from wearable sensors to classify diseases such as heart disease, diabetes, etc., based on sensor data. The goal is to predict whether a patient is diseased (1) or healthy (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e39608c",
      "metadata": {
        "id": "0e39608c"
      },
      "source": [
        "### Step 1: Problem Definition\n",
        "\n",
        "We aim to predict whether a patient has a particular disease (e.g., heart disease, diabetes) based on sensor data collected from wearable devices (e.g., accelerometer, heart rate monitors, temperature sensors). The goal is to classify the data into diseased (1) or healthy (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1faa2158",
      "metadata": {
        "id": "1faa2158"
      },
      "source": [
        "### Step 2: Dataset Preparation\n",
        "\n",
        "We need a dataset with sensor data features such as:\n",
        "\n",
        "- Heart rate\n",
        "- Steps taken\n",
        "- Temperature\n",
        "- Activity level (e.g., walking, running)\n",
        "- Sleep duration\n",
        "\n",
        "The labels are binary: 0 (healthy) and 1 (diseased)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7acc8b",
      "metadata": {
        "id": "5f7acc8b"
      },
      "source": [
        "#### Step 2.1: Prepare a Sample Dataset\n",
        "\n",
        "In practice, you would use a real healthcare or sensor dataset. For demonstration, we will use a hypothetical dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda0c25b",
      "metadata": {
        "id": "eda0c25b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'heart_rate': [72, 80, 78, 65, 85, 92, 68, 76, 77, 79],\n",
        "    'steps': [1000, 1500, 1200, 800, 1800, 2000, 900, 1000, 1100, 1300],\n",
        "    'temperature': [36.5, 37.0, 36.8, 36.4, 37.1, 37.5, 36.6, 36.7, 36.9, 37.2],\n",
        "    'activity_level': [1, 3, 2, 1, 4, 5, 1, 2, 3, 4],  # 1 = sedentary, 5 = active\n",
        "    'sleep_duration': [7, 6, 7, 8, 5, 4, 8, 7, 6, 5],  # hours of sleep\n",
        "    'label': [0, 1, 0, 0, 1, 1, 0, 0, 1, 0]  # 0 = healthy, 1 = diseased\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target variable\n",
        "X = df.drop('label', axis=1)  # features\n",
        "y = df['label']  # target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Data: 1000 Samples"
      ],
      "metadata": {
        "id": "4TXME6Jjill0"
      },
      "id": "4TXME6Jjill0"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Generate random data for 1000 samples\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulate data\n",
        "heart_rate = np.random.randint(60, 100, 1000)\n",
        "steps = np.random.randint(500, 2500, 1000)\n",
        "temperature = np.random.uniform(36.0, 37.5, 1000)\n",
        "activity_level = np.random.randint(1, 6, 1000)  # 1 = sedentary, 5 = active\n",
        "sleep_duration = np.random.randint(4, 9, 1000)  # hours of sleep\n",
        "label = np.random.choice([0, 1], 1000, p=[0.7, 0.3])  # 0 = healthy, 1 = diseased\n",
        "\n",
        "# Create DataFrame\n",
        "data = {\n",
        "    'heart_rate': heart_rate,\n",
        "    'steps': steps,\n",
        "    'temperature': temperature,\n",
        "    'activity_level': activity_level,\n",
        "    'sleep_duration': sleep_duration,\n",
        "    'label': label\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target variable\n",
        "X = df.drop('label', axis=1)  # features\n",
        "y = df['label']  # target variable\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h10-LBm1ioPq"
      },
      "id": "h10-LBm1ioPq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aa88296e",
      "metadata": {
        "id": "aa88296e"
      },
      "source": [
        "### Step 3: Data Preprocessing & Normalization\n",
        "\n",
        "Sensor data often varies in scale, so normalization or standardization is important to ensure all features contribute equally to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7af3f1",
      "metadata": {
        "id": "ef7af3f1"
      },
      "source": [
        "#### Step 3.1: Normalize the Data Using StandardScaler\n",
        "\n",
        "We use the `StandardScaler` to transform the features into a similar scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad51b31",
      "metadata": {
        "id": "8ad51b31"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Verify the scaling\n",
        "print(pd.DataFrame(X_scaled, columns=X.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "293908e6",
      "metadata": {
        "id": "293908e6"
      },
      "source": [
        "### Step 4: Split the Data into Training and Testing Sets\n",
        "\n",
        "We need to split the data into training and testing sets to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9408bb7",
      "metadata": {
        "id": "f9408bb7"
      },
      "source": [
        "#### Step 4.1: Split the Data Using `train_test_split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d29b10ab",
      "metadata": {
        "id": "d29b10ab"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723a55ad",
      "metadata": {
        "id": "723a55ad"
      },
      "source": [
        "### Step 5: Train the Logistic Regression Model\n",
        "\n",
        "Now, we can train the logistic regression model on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3549ef8c",
      "metadata": {
        "id": "3549ef8c"
      },
      "source": [
        "#### Step 5.1: Initialize and Fit the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2edba862",
      "metadata": {
        "id": "2edba862"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d854516",
      "metadata": {
        "id": "9d854516"
      },
      "source": [
        "### Step 6: Make Predictions\n",
        "\n",
        "After training the model, we can use it to make predictions on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ee2296",
      "metadata": {
        "id": "98ee2296"
      },
      "source": [
        "#### Step 6.1: Predict Disease Classification for the Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f274295",
      "metadata": {
        "id": "5f274295"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Display the predictions\n",
        "print(\"Predictions:\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb5914b",
      "metadata": {
        "id": "ceb5914b"
      },
      "source": [
        "### Step 7: Model Evaluation\n",
        "\n",
        "We can evaluate the performance of the model using accuracy, precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfa1b5f",
      "metadata": {
        "id": "7bfa1b5f"
      },
      "source": [
        "#### Step 7.1: Evaluate the Model Using `classification_report` and `accuracy_score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1606c875",
      "metadata": {
        "id": "1606c875"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f3fe718",
      "metadata": {
        "id": "6f3fe718"
      },
      "source": [
        "### Step 8: Confusion Matrix\n",
        "\n",
        "A confusion matrix will help us visualize the performance of the model by showing the true positives, true negatives, false positives, and false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f8c294b",
      "metadata": {
        "id": "0f8c294b"
      },
      "source": [
        "#### Step 8.1: Generate and Display the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad604bb",
      "metadata": {
        "id": "8ad604bb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(\"Confusion Matrix:\", conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}